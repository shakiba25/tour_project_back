# tours/utils/chat_logic.py
import traceback
from tours.utils.query_searcher import search_tour_chunks
from tours.utils.query_filter import get_chunks_for_query, safe_get_answer
import openai
# from django.conf import settings


# MODEL = "qwen/qwen3-235b-a22b:free"  # ÛŒØ§ Ù‡Ø± Ù…Ø¯Ù„ÛŒ Ú©Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
# MODEL = "google/gemini-2.0-flash-exp:free"
MODEL = "microsoft/mai-ds-r1:free" #Ø®ÛŒÙ„ÛŒ 
# MODEL = "nvidia/llama-3.1-nemotron-ultra-253b-v1:free" #Ø¨Ø¯ 
MODEL = "moonshotai/kimi-dev-72b:free" #Ø®ÙˆØ¨
# MODEL = "nousresearch/deephermes-3-llama-3-8b-preview:free"
MODEL = "google/gemma-3-27b-it:free"  #Ø®ÛŒÙ„ÛŒ 

openai.api_key = "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"
openai.api_base = "https://openrouter.ai/api/v1" 

def generate_assistant_response(chat_session, user_content):
    """
    chat_session: ChatSession instance
    user_content: Ù…ØªÙ† Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± (string)
    
    Ø¨Ø§Ø² Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯: Ù…ØªÙ† Ù¾Ø§Ø³Ø® Ø¯Ø³ØªÛŒØ§Ø± (string)
    """
    # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ
    history = [f"{msg.role}: {msg.content}" for msg in chat_session.messages.all()]

    # ÙÛŒÙ„ØªØ± Ùˆ Ø¬Ø³ØªØ¬ÙˆÛŒ ØªÙˆØ±Ù‡Ø§
    filtered_chunks = get_chunks_for_query(user_content)
    # filtered_chunks = []

    print("------------------------------filtered_chunks---------------------------")    
    print(filtered_chunks)    
    
    retrieved = search_tour_chunks(user_content, top_k=5)

    print("------------------------------retrieved---------------------------")    
    print(retrieved)    

    # Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª
    prompt = "ğŸ“Œ Ù„ÛŒØ³Øª ØªÙˆØ±Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª. Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†:\n\n"
    prompt += "âœ… Ù†ØªØ§ÛŒØ¬ ÙÛŒÙ„ØªØ±:\n"
    for i, c in enumerate(filtered_chunks, 1):
        if hasattr(c, "text"):
            prompt += f"{i}. {c.text}\n"
        elif isinstance(c, dict) and "chunk" in c:
            prompt += f"{i}. {c['chunk'].text}\n"
        else:
            prompt += f"{i}. {str(c)}\n"

    prompt += "\nğŸ” Ù†ØªØ§ÛŒØ¬ Ù…Ø´Ø§Ø¨Ù‡Øª:\n"
    for i, r in enumerate(retrieved, 1):
        chunk = r["chunk"] if isinstance(r, dict) else r
        prompt += f"{i}. {chunk.text}\n"

    prompt += "\nğŸ•‘ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ:\n"
    for turn in history:
        prompt += f"- {turn}\n"

    prompt += f"\nâ“ Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±: {user_content}\n"
    prompt += " Ø¨Ù‡ ØµÙˆØ±Øª Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø±ÙˆØ§Ù† Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡ØŒ Ù…Ø«Ù„ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø³ÙØ±. Ø¬ÙˆØ§Ø¨ Ø±Ùˆ Ú©ÙˆØªØ§Ù‡ Ùˆ Ø³Ø§Ø¯Ù‡ Ù†Ú¯Ù‡ Ø¯Ø§Ø± ØªØ§ Ø¨Ù‡ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªÙˆÚ©Ù† Ù†Ø®ÙˆØ±ÛŒ. Ø§Ú¯Ø± ØªÙˆØ± Ø®Ø§ØµÛŒ Ù‡Ø³Øª Ø§Ø³Ù…Ø´Ùˆ Ø¨ÛŒØ§Ø±."
    # prompt += " </think> Ø±Ø§ Ø­Ø°Ù Ú©Ù† Ø§Ø² Ù¾Ø§Ø³Ø®Øª Ùˆ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡"
    print("------------------------------prompt---------------------------")    
    print(prompt)    
    
    # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ LLM
    try:
        response = openai.ChatCompletion.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.7,
            top_p=1,
        )
        answer = safe_get_answer(response)
    except Exception as e:
        traceback.print_exc()
        answer = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² Ù…Ø¯Ù„: {str(e)}"
    
    print("------------------------------prompt---------------------------")    

    print(answer)
    return answer





#shakina jadida
# openai.api_key = "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"