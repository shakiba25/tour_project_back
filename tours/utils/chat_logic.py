# tours/utils/chat_logic.py
import traceback
from tours.utils.query_searcher import search_tour_chunks
from tours.utils.query_filter import get_chunks_for_query
from tours.utils.rewrite_prompt import rewrite_query_with_context

import openai
# from django.conf import settings

# MODEL = "google/gemma-3-27b-it:free"  #ุฎู  ุงูู ุงูู ฺฉู ุงุตูู
# MODEL = "google/gemma-3n-e4b-it:free"


# MODEL = "qwen/qwen3-235b-a22b:free"  # ุง ูุฑ ูุฏู ฺฉู ุงุณุชูุงุฏู ูโฺฉูุฏ
# MODEL = "google/gemini-2.0-flash-exp:free"

# MODEL = "nvidia/llama-3.1-nemotron-ultra-253b-v1:free" #ุจุฏ 
# MODEL = "moonshotai/kimi-dev-72b:free" #ุฎูุจ
# MODEL = "nousresearch/deephermes-3-llama-3-8b-preview:free"
# MODEL = "google/gemma-3-27b-it:free"  #ุฎู  ุงูู ุงูู ฺฉู ุงุตูู
# MODEL = "tngtech/deepseek-r1t2-chimera:free"  #ุฎูุจ
# MODEL = "microsoft/mai-ds-r1:free" # / ุฎุฑุฑุงุจ / ุฎู 
# MODEL = "meta-llama/llama-3.3-8b-instruct:free"
# MODEL = "qwen/qwen3-8b:free"


# openai.api_key = "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"
# openai.api_base = "https://openrouter.ai/api/v1" 

from openai import OpenAI

# --------------------- ุชูุธู ฺฉูุงูุช --------------------- #
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="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",  # ฺฉูุฏ ุฌุฏุฏุช ุฑู ุงูุฌุง ุจุฐุงุฑ
    # api_key = "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", #zapas


)




def generate_assistant_response(chat_session, user_content):
    """
    chat_session: ChatSession instance
    user_content: ูุชู ูพุงู ฺฉุงุฑุจุฑ (string)
    
    ุจุงุฒ ูโฺฏุฑุฏุงูุฏ: ูุชู ูพุงุณุฎ ุฏุณุชุงุฑ (string)
    """
    # ุชุงุฑุฎฺู ฺฏูุชฺฏู
    history = [f"{msg.role}: {msg.content}" for msg in chat_session.messages.all()]
    # print("\n history ------------------- \n ")
    # print(history)
    # print("\n \n ")
    # ุจุงุฒููุณ ุณูุงู ููุท ุงฺฏุฑ ูุณุชูุฑ ูุฌูุฏ ุฏุงุดุชู ุจุงุดุฏ
    # rewritten_query = user_content
    if len(history) > 1:
        rewritten_query = rewrite_query_with_context(
            user_content=user_content,
            history=history,
            use_model=True 
        )
    else:
        rewritten_query = user_content
    
# ============================================== user_content => rewritten_query
    print(f"\n \n $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ \n\n {rewritten_query} \n\n")

# ==============================
    # ููุชุฑ ู ุฌุณุชุฌู ุชูุฑูุง
    filtered_tours , filtered_chunks = get_chunks_for_query(rewritten_query)  # ุงูุฌุง
    # filtered_chunks = []

    print("------------------------------filtered_chunks---------------------------")    
    print(filtered_chunks)    
    
    retrieved = search_tour_chunks(rewritten_query, top_faq=2, top_tour=5)      # ุงูุฌุง

    print("------------------------------retrieved---------------------------")    
    print(retrieved)    

    # ุณุงุฎุช ูพุฑุงููพุช
    prompt = "๐ ูุณุช ุชูุฑูุง ูุฑุชุจุท ููุฌูุฏ ุงุณุช. ุงุฒ ุงุทูุงุนุงุช ุฒุฑ ุงุณุชูุงุฏู ฺฉู:\n\n"
    prompt += "โ ูุชุงุฌ ููุชุฑ:\n"
    for i, c in enumerate(filtered_chunks, 1):
        if hasattr(c, "text"):
            prompt += f"{i}. {c.text}\n"
        elif isinstance(c, dict) and "chunk" in c:
            prompt += f"{i}. {c['chunk'].text}\n"
        else:
            prompt += f"{i}. {str(c)}\n"

    prompt += "\n๐ ูุชุงุฌ ูุดุงุจูุช:\n"
    for i, r in enumerate(retrieved, 1):
        chunk = r["chunk"] if isinstance(r, dict) else r
        prompt += f"{i}. {chunk.text}\n"

    prompt += "\n๐ ุชุงุฑุฎฺู ฺฏูุชฺฏู:\n"
    for turn in history:
        prompt += f"- {turn}\n"

    prompt += f"\nโ ุณูุงู ุฌุฏุฏ ฺฉุงุฑุจุฑ: {user_content}\n"

    # prompt += " ุญุฏุงฺฉุซุฑ ูพุงุณุฎ ฺฉู ูุฏู 150 ุชูฺฉู ุจุงุดุฏ ุงฺฏุฑ ุจุดุชุฑ ุดุฏ ุฎูุงุตู ฺฉู ู ูพุงุณุฎ ุจุฏู"
    prompt += "ูุทูุงู ูพุงุณุฎ ุฑุง ุญุฏุงฺฉุซุฑ ุฏุฑ 150 ุชูฺฉู ุจููุณุ ุฎู ุฎูุงุตู ู ููุฏุ ููุท ูฺฉุงุช ฺฉูุฏ ุฑุง ุจฺฏูุ ุงฺฏุฑ ฺูุฏู ุชูุฑ ุจูุฏ ููุท ูุงู ู ููุชโุดุงู ุฑุง ุจู ุตูุฑุช ูุณุช ฺฉูุชุงู ุจุงูุฑ."

    prompt += " ุจู ุตูุฑุช ุทุจุน ู ุฑูุงู ุงูุง ุฎูุงุตู ู ููุฏ ู ฺฉูุชุงู ุฌูุงุจ ุจุฏูุ ูุซู ฺฉ ุฏุณุชุงุฑ ุณูุฑ. ุงฺฏุฑ ุชูุฑ ุฎุงุต ูุณุช ุงุณูุดู ุจุงุฑ."
    # prompt += " ุงฺฏุฑ ุฏุฑุจุงุฑู  ุฌุฒุงุช ฺฉ ุชูุฑ ูพุฑุณุฏ ุชู ุชูุงู ุฌุฒุงุช ุขู ุฑุง(ููุช-ูุฒูู-ุชุงุฑุฎ ู ุณุงุนุช ุฑูุช ู ุจุฑฺฏุดุช) ุจฺฏู ุงฺฏุฑ ุงุทูุงุนุงุช ูุฏุงุดุช ูฺฏู ุงุณู ุขู ุงุทูุงุนุงุช ุฑุง "
    # prompt += " ุงฺฏุฑ ุฏุฑุจุงุฑู  ุฌุฒุงุช ฺฉ ุชูุฑ ูพุฑุณุฏ ุชู ุชูุงู ุฌุฒุงุช ุขู ุฑุง(ููุช-ูุฒูู-ุชุงุฑุฎ ู ุณุงุนุช ุฑูุช ู ุจุฑฺฏุดุช) ุจฺฏู  "
    # prompt += " ุงฺฏุฑ ุณูุงู ุฏุฑ ููุฑุฏ ุชูุฑ ุฎุงุต ูุจูุฏ ุงุทูุงุนุงุช ฺฉ ุชูุฑ ุฑุง ุงูฺฉ ุชฺฉุฑุงุฑ ูฺฉู "
    prompt += " ุงฺฏุฑ ฺูุฏู ุชูุฑ ุฎูุงุณุชู ุจูุฏ ูพุงุณุฎุช ุดุงูู ูุณุช ุฎู ุฎูุงุตู ู ฺฉูุชุงู ุงุฒ ููู  ุชูุฑ ูุง ูุชุงุฌ ููุชุฑ ุดุฏู ุจุงุดุฏ  "
    # prompt += " ุงฺฏุฑ ฺูุฏู ุชูุฑ ุจูุฏ ุจู ุตูุฑุช ูุณุช ุจฺฏู  "
    # prompt += " ุฏุฑ ุชุจุฏู ูุงู ุชุงุฑุฎ ูุง ุฏูุช ฺฉู ุ ูุงู ูุง ุดูุณ ูุณุชูุฏ ูุซูุง YYYY/06/DD ุนู ุดูุฑูุฑ"
    prompt += "ุดูุณ ูุทูุง ุชุงุฑุฎโูุง ุฑุง ุฏููุงู ูุซู ุงู ูุงูุจ ุดูุณ (YYYY/MM/DD) ููุงุด ุจุฏู ู ุชุจุฏู ุจู ูุชู ูฺฉู."  
    prompt += " ููฺฉโูุง ูุจูุงู ุฏุฑ ุงุทูุงุนุงุช ุชูุฑูุง ูุฌูุฏ ุฏุงุฑูุฏุ ุขููุง ุฑุง ุนูุงู ฺฉูพ ฺฉู."
    prompt += " ูุซุงู: ุงฺฏุฑ ุงุทูุงุนุงุช ุชูุฑ ุดุงูู '<a href='http://localhost:5173/tours/tour_001'>ุชูุฑ ฺฉุด</a>' ุจุงุดุฏุ ุฏููุงู ููู ุฑุง ุฏุฑ ูพุงุณุฎ ุจุงูุฑ."
    prompt += "ุงฺฏุฑ ููฺฉ ูุจูุฏ ุง ูุงูุต ุจูุฏ ุงุตูุง ูููุณ ฺฉู ููฺฉ ููุฌูุฏ ุฏุฑ ูพุงุณุฎุช"
    prompt += "ุงุฒ enter ูุง ุจุฑุง ุฌุฏุงุณุงุฒ ุจุฎุด ูุง ุฌูุงุจุช ุจุฑุง ูุฑุชุจ ุชุฑ ุดุฏู ุงุณุชูุงุฏู ฺฉู"  

    print("------------------------------prompt---------------------------")    
    print(prompt)    
    
    
    
    
    # ุฏุฑุฎูุงุณุช ุจู LLM
    try:
        completion = client.chat.completions.create(
            # model = "google/gemma-3n-e4b-it:free",  # ูุฏู ููุฑุฏ ูุธุฑ
            # model = "deepseek/deepseek-r1-0528-qwen3-8b:free",
            # model = "qwen/qwen3-235b-a22b:free",
            # model = "qwen/qwen3-4b:free",
            # model = "qwen/qwen3-8b:free",
            # model = "qwen/qwen3-14b:free",
            
            # model = "mistralai/mistral-7b-instruct:free",
            # model = "google/gemma-2-9b-it:free",  # ุงุฎุฑ ุงูุจูุฏ
            
            # model = "x-ai/grok-4-fast:free", # ุญู ุฎูุจ ุจุนุฏ llm
            
            # model = "deepseek/deepseek-chat-v3.1:free", # ุจุฏ ูุณุช
            # model = "microsoft/mai-ds-r1:free", # ุจุฏ ุทูู ุงุฑูุฑ
            # model = "openai/gpt-oss-20b:free", #  ุฎุฑุงุจู ุฌูุงุจ ูุตูู
            
            # model = "meta-llama/llama-3.3-8b-instruct:free", #ุจุฏ 
            # model = "meta-llama/llama-4-maverick:free",
            model = "meta-llama/llama-3.3-70b-instruct:free", #  ฺฉุงููุง ุฏุฑุณุช ุฎู ุฎูุจ
            
            # model ="mistralai/mistral-nemo:free",
            # model = "google/gemma-3-27b-it:free",
            # model = "google/gemma-3-12b-it:free" ,
            # model = "google/gemini-2.0-flash-exp:free",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=400,
            temperature=0.3,
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        traceback.print_exc()
        answer = f"โ ุฎุทุง ุฏุฑ ุฏุฑุงูุช ูพุงุณุฎ ุงุฒ ูุฏู: {str(e)}"
    
    print("------------------------------prompt---------------------------")    

    print(answer)
    return answer





#shakina jadida
# openai.api_key = "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"
# opneai.api_key = "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"