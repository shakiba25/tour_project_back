# tours/utils/chat_logic.py
import traceback
from tours.utils.query_searcher import search_tour_chunks
from tours.utils.query_filter import get_chunks_for_query, safe_get_answer
from tours.utils.rewrite_prompt import rewrite_query_with_context

import openai
# from django.conf import settings

MODEL = "google/gemma-3-27b-it:free"  #Ø®ÛŒÙ„ÛŒ  Ø§ÛŒÙ†Ù‡ Ø§ÙˆÙ†ÛŒ Ú©Ù‡ Ø§ØµÙ„ÛŒÙ‡
# MODEL = "google/gemma-3n-e4b-it:free"


# MODEL = "qwen/qwen3-235b-a22b:free"  # ÛŒØ§ Ù‡Ø± Ù…Ø¯Ù„ÛŒ Ú©Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
# MODEL = "google/gemini-2.0-flash-exp:free"

# MODEL = "nvidia/llama-3.1-nemotron-ultra-253b-v1:free" #Ø¨Ø¯ 
# MODEL = "moonshotai/kimi-dev-72b:free" #Ø®ÙˆØ¨
# MODEL = "nousresearch/deephermes-3-llama-3-8b-preview:free"
# MODEL = "google/gemma-3-27b-it:free"  #Ø®ÛŒÙ„ÛŒ  Ø§ÛŒÙ†Ù‡ Ø§ÙˆÙ†ÛŒ Ú©Ù‡ Ø§ØµÙ„ÛŒÙ‡
# MODEL = "tngtech/deepseek-r1t2-chimera:free"  #Ø®ÙˆØ¨
# MODEL = "microsoft/mai-ds-r1:free" # / Ø®Ø±Ø±Ø§Ø¨ / Ø®ÛŒÙ„ÛŒ 
# MODEL = "meta-llama/llama-3.3-8b-instruct:free"
# MODEL = "qwen/qwen3-8b:free"


openai.api_key = "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"
openai.api_base = "https://openrouter.ai/api/v1" 

def generate_assistant_response(chat_session, user_content):
    """
    chat_session: ChatSession instance
    user_content: Ù…ØªÙ† Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø± (string)
    
    Ø¨Ø§Ø² Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯: Ù…ØªÙ† Ù¾Ø§Ø³Ø® Ø¯Ø³ØªÛŒØ§Ø± (string)
    """
    # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ
    history = [f"{msg.role}: {msg.content}" for msg in chat_session.messages.all()]

    # Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ ÙÙ‚Ø· Ø§Ú¯Ø± Ù‡ÛŒØ³ØªÙˆØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
    # rewritten_query = user_content
    if history:
        rewritten_query = rewrite_query_with_context(
            user_content=user_content,
            history=history,
            use_model=True  # Ø§Ú¯Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ø¨Ø¯ÙˆÙ† GPT Ø¨Ø§Ø´Ù‡ Ø¨Ø°Ø§Ø± False
        )
    else:
        rewritten_query = user_content
    
# ============================================== user_content => rewritten_query
    print(f"\n \n $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ \n\n {rewritten_query} \n\n")

# ==============================
    # ÙÛŒÙ„ØªØ± Ùˆ Ø¬Ø³ØªØ¬ÙˆÛŒ ØªÙˆØ±Ù‡Ø§
    filtered_tours , filtered_chunks = get_chunks_for_query(rewritten_query)  # Ø§ÛŒÙ†Ø¬Ø§
    # filtered_chunks = []

    print("------------------------------filtered_chunks---------------------------")    
    print(filtered_chunks)    
    
    retrieved = search_tour_chunks(rewritten_query, top_k=5)      # Ø§ÛŒÙ†Ø¬Ø§

    print("------------------------------retrieved---------------------------")    
    print(retrieved)    

    # Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª
    prompt = "ğŸ“Œ Ù„ÛŒØ³Øª ØªÙˆØ±Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª. Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†:\n\n"
    prompt += "âœ… Ù†ØªØ§ÛŒØ¬ ÙÛŒÙ„ØªØ±:\n"
    for i, c in enumerate(filtered_chunks, 1):
        if hasattr(c, "text"):
            prompt += f"{i}. {c.text}\n"
        elif isinstance(c, dict) and "chunk" in c:
            prompt += f"{i}. {c['chunk'].text}\n"
        else:
            prompt += f"{i}. {str(c)}\n"

    prompt += "\nğŸ” Ù†ØªØ§ÛŒØ¬ Ù…Ø´Ø§Ø¨Ù‡Øª:\n"
    for i, r in enumerate(retrieved, 1):
        chunk = r["chunk"] if isinstance(r, dict) else r
        prompt += f"{i}. {chunk.text}\n"

    prompt += "\nğŸ•‘ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ:\n"
    for turn in history:
        prompt += f"- {turn}\n"

    prompt += f"\nâ“ Ø³ÙˆØ§Ù„ Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±: {user_content}\n"
    prompt += " Ø¨Ù‡ ØµÙˆØ±Øª Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø±ÙˆØ§Ù† Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡ØŒ Ù…Ø«Ù„ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ø³ÙØ±. Ø¬ÙˆØ§Ø¨ Ø±Ùˆ Ú©ÙˆØªØ§Ù‡ Ùˆ Ø³Ø§Ø¯Ù‡ Ù†Ú¯Ù‡ Ø¯Ø§Ø± ØªØ§ Ø¨Ù‡ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªÙˆÚ©Ù† Ù†Ø®ÙˆØ±ÛŒ. Ø§Ú¯Ø± ØªÙˆØ± Ø®Ø§ØµÛŒ Ù‡Ø³Øª Ø§Ø³Ù…Ø´Ùˆ Ø¨ÛŒØ§Ø±."
    # prompt += " Ø§Ú¯Ø± Ø¯Ø±Ø¨Ø§Ø±Ù‡ ÛŒ Ø¬Ø²ÛŒÛŒØ§Øª ÛŒÚ© ØªÙˆØ± Ù¾Ø±Ø³ÛŒØ¯ ØªÙˆ ØªÙ…Ø§Ù… Ø¬Ø²ÛŒÛŒØ§Øª Ø¢Ù† Ø±Ø§(Ù‚ÛŒÙ…Øª-Ù‡Ø²ÛŒÙ†Ù‡-ØªØ§Ø±ÛŒØ® Ùˆ Ø³Ø§Ø¹Øª Ø±ÙØª Ùˆ Ø¨Ø±Ú¯Ø´Øª) Ø¨Ú¯Ùˆ Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù†Ø¯Ø§Ø´ØªÛŒ Ù†Ú¯Ùˆ Ø§Ø³Ù… Ø¢Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ "
    prompt += " Ø§Ú¯Ø± Ø¯Ø±Ø¨Ø§Ø±Ù‡ ÛŒ Ø¬Ø²ÛŒÛŒØ§Øª ÛŒÚ© ØªÙˆØ± Ù¾Ø±Ø³ÛŒØ¯ ØªÙˆ ØªÙ…Ø§Ù… Ø¬Ø²ÛŒÛŒØ§Øª Ø¢Ù† Ø±Ø§(Ù‚ÛŒÙ…Øª-Ù‡Ø²ÛŒÙ†Ù‡-ØªØ§Ø±ÛŒØ® Ùˆ Ø³Ø§Ø¹Øª Ø±ÙØª Ùˆ Ø¨Ø±Ú¯Ø´Øª) Ø¨Ú¯Ùˆ  "
    # prompt += " Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù…Ø§Ù‡ ØªØ§Ø±ÛŒØ® Ù‡Ø§ Ø¯Ù‚Øª Ú©Ù† ØŒ Ù…Ø§Ù‡ Ù‡Ø§ Ø´Ù…Ø³ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ù…Ø«Ù„Ø§ YYYY/06/DD ÛŒØ¹Ù†ÛŒ Ø´Ù‡Ø±ÛŒÙˆØ±"
    prompt += " Ù„Ø·ÙØ§ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù…Ø«Ù„ Ø§ÛŒÙ† Ù‚Ø§Ù„Ø¨ (YYYY/MM/DD) Ù†Ù…Ø§ÛŒØ´ Ø¨Ø¯Ù‡ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ØªÙ† Ù†Ú©Ù†."  
    print("------------------------------prompt---------------------------")    
    print(prompt)    
    
    # Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ LLM
    try:
        response = openai.ChatCompletion.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.7,
            top_p=1,
        )
        answer = safe_get_answer(response)
    except Exception as e:
        traceback.print_exc()
        answer = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² Ù…Ø¯Ù„: {str(e)}"
    
    print("------------------------------prompt---------------------------")    

    print(answer)
    return answer





#shakina jadida
# openai.api_key = "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"