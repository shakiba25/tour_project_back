import sys
import os

BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(BASE_DIR)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "travelproj.settings")

import django
django.setup()


# api_key="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"  


from openai import OpenAI

# --------------------- ØªÙ†Ø¸ÛŒÙ… Ú©Ù„Ø§ÛŒÙ†Øª --------------------- #
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="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",
    # api_key = "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", #zapas

)

def rewrite_query_with_context(user_content, history, use_model=True):
    """
    Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.
    
    - history: Ù„ÛŒØ³Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ú¯ÙØªÚ¯Ùˆ (Ù…Ø«Ù„ ["user: ...", "assistant: ..."])
    - use_model: Ø§Ú¯Ø± True Ø¨Ø§Ø´Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´Ù‡
    """
    if not use_model:
        return user_content

    history_str = "\n".join(history[-4:])

    prompt = f"""Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± ÛŒÚ© Ú¯ÙØªÚ¯ÙˆÛŒ Ø³ÙØ± Ø§Ø³Øª. Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±Ø´ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù† Ø¨Ù‡ Ø·ÙˆØ±ÛŒ Ú©Ù‡ Ù…Ø³ØªÙ‚Ù„ Ùˆ ÙˆØ§Ø¶Ø­ Ø¨Ø§Ø´Ø¯ Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù‚Ø¨Ù„ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.

ğŸ•‘ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ:
{history_str}

â“ Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±:
{user_content}

ğŸ” Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ ÙˆØ§Ø¶Ø­:"""

    try:
        completion = client.chat.completions.create(
            model = "google/gemma-3n-e4b-it:free",  # Ù…Ø¯Ù„ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100,
            temperature=0.3,
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„:", e)
        return user_content  
    
    

if __name__ == "__main__":
    history = [
        "user: ØªÙˆØ± Ú©ÛŒØ´ Ú†Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ Ù‡Ø³ØªØŸ",
        "assistant: ØªØ§Ø±ÛŒØ® Ø±ÙØª ØªÙˆØ± Ú©ÛŒØ´ 10-08-1404 Ø³Ø§Ø¹Øª 12 Ø§Ø³Øª",
    ]

    user_question = "Ø®Ø¯Ù…Ø§ØªØ´ Ú†ÛŒÙ‡ØŸ Ø§Ø² Ú†Ù‡ Ø¬Ø§Ù‡Ø§ÛŒÛŒØ´ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù…ÛŒÚ©Ù†ÛŒÙ…ØŸ"

    result = rewrite_query_with_context(user_question, history, use_model=True)
    
    print("âœ… Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ:")
    print(result)
    
# âœ… Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ:
# Ø®Ø¯Ù…Ø§Øª Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± ØªÙˆØ± Ú©ÛŒØ´ Ú†ÛŒØ³Øª Ùˆ Ø¯Ø± Ø§ÛŒÙ† ØªÙˆØ± Ø§Ø² Ú†Ù‡ Ù…Ú©Ø§Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯ØŸ

    
# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ø² Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ
# model_path = r"C:\Users\Asus\Desktop\test t5\models\bloomz-1b1"
# tokenizer = AutoTokenizer.from_pretrained(model_path)
# model = AutoModelForCausalLM.from_pretrained(model_path)
# model.eval()  # Ø­Ø§Ù„Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ

# def rewrite_query_with_context(user_content, history, use_model=True):
#     """
#     Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.
    
#     - history: Ù„ÛŒØ³Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ú¯ÙØªÚ¯Ùˆ (Ù…Ø«Ù„ ["user: ...", "assistant: ..."])
#     - use_model: Ø§Ú¯Ø± True Ø¨Ø§Ø´Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ LLM Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´Ù‡
#     """
#     if not use_model:
#         return user_content

#     history_str = "\n".join(history[-4:])

#     prompt = f"""Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± ÛŒÚ© Ú¯ÙØªÚ¯ÙˆÛŒ Ø³ÙØ± Ø§Ø³Øª. Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±Ø´ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù† Ø¨Ù‡ Ø·ÙˆØ±ÛŒ Ú©Ù‡ Ù…Ø³ØªÙ‚Ù„ Ùˆ ÙˆØ§Ø¶Ø­ Ø¨Ø§Ø´Ø¯ Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù‚Ø¨Ù„ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.

# ğŸ•‘ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ:
# {history_str}

# â“ Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±:
# {user_content}

# ğŸ” Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ ÙˆØ§Ø¶Ø­:"""

#     # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ tensor
#     inputs = tokenizer(prompt, return_tensors="pt")
    
#     # Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ (ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†)
#     with torch.no_grad():
#         outputs = model.generate(
#             **inputs,
#             max_new_tokens=100,
#             do_sample=True,
#             temperature=0.3,
#             top_p=0.9,
#             eos_token_id=tokenizer.eos_token_id,
#             pad_token_id=tokenizer.pad_token_id
#         )
    
#     # Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®
#     generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
#     # Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø® Ø§Ø² prompt (Ù…Ù…Ú©Ù†Ù‡ Ø¨Ø®ÙˆØ§ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¬Ø¯Ø§ Ú©Ù†ÛŒ)
#     answer = generated_text[len(prompt):].strip()

#     return answer


# # ----------mt5--------------------------------------------------------------------------


# from transformers import MT5Tokenizer, TFMT5ForConditionalGeneration
# import tensorflow as tf

# # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„
# model_path = r"C:\Users\Asus\Desktop\test t5\models\mt5-large"

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ (TensorFlow)
# tokenizer = MT5Tokenizer.from_pretrained(model_path)
# model = TFMT5ForConditionalGeneration.from_pretrained(model_path)

# def rewrite_query_with_context(user_content, history, use_model=True):
#     """
#     Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ ØªÙˆØ³Ø· Ù…Ø¯Ù„ MT5 (TensorFlow)

#     Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
#     - user_content (str): Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±
#     - history (list of str): ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ (Ù…Ø«Ù„ ["user: ...", "assistant: ..."])
#     - use_model (bool): Ø§Ú¯Ø± False Ø¨Ø§Ø´Ù‡ Ù‡Ù…ÙˆÙ† Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ Ø±Ùˆ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÙˆÙ†Ù‡

#     Ø®Ø±ÙˆØ¬ÛŒ:
#     - str: Ø³ÙˆØ§Ù„ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ ÛŒØ§ Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ
#     """
#     if not use_model:
#         return user_content

#     # ØªØ±Ú©ÛŒØ¨ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ùˆ Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª
#     history_str = "\n".join(history[-4:])  # Ø¢Ø®Ø±ÛŒÙ† Û´ Ù¾ÛŒØ§Ù…
#     prompt = f"rewrite this user query independently based on the following chat history.\n\nhistory:\n{history_str}\n\nquestion:\n{user_content}\n\nrewritten question:"

#     # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ (max length Ø±Ø¹Ø§ÛŒØª Ø´ÙˆØ¯)
#     inputs = tokenizer(prompt, return_tensors="tf", max_length=512, truncation=True, padding="max_length")

#     # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
#     outputs = model.generate(
#         input_ids=inputs["input_ids"],
#         attention_mask=inputs["attention_mask"],
#         max_length=100,
#         num_beams=4,
#         no_repeat_ngram_size=2,
#         early_stopping=True
#     )

#     # Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®
#     rewritten = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
#     return rewritten


# # ----------flan--------------------------------------------------------------------------


# from transformers import T5ForConditionalGeneration, T5Tokenizer
# import torch

# # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„
# model_path = r"C:\Users\Asus\Desktop\project-final\medical\models\flan-t5-base"

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
# tokenizer = T5Tokenizer.from_pretrained(model_path)
# model = T5ForConditionalGeneration.from_pretrained(model_path)

# # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ GPU Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)
# model.eval()

# def rewrite_query_with_context3(user_content, history, use_model=True):
#     """
#     Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.
    
#     Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
#         user_content (str): Ø³ÙˆØ§Ù„ Ø¢Ø®Ø± Ú©Ø§Ø±Ø¨Ø±
#         history (list of str): ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ (Ù…Ø«Ù„Ø§ ["user: ...", "assistant: ..."])
#         use_model (bool): Ø§Ú¯Ø± True Ø¨Ø§Ø´Ù‡ØŒ Ø§Ø² Ù…Ø¯Ù„ FLAN-T5 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ ÙˆÚ¯Ø±Ù†Ù‡ Ø³ÙˆØ§Ù„ Ø±Ùˆ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÙˆÙ†Ù‡
    
#     Ø®Ø±ÙˆØ¬ÛŒ:
#         str: Ø³ÙˆØ§Ù„ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ ÛŒØ§ Ø§ØµÙ„ÛŒ
#     """

#     if not use_model:
#         return user_content

#     # ÙÙ‚Ø· Û´ Ù¾ÛŒØ§Ù… Ø¢Ø®Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
#     history_str = "\n".join(history[-4:])

#     prompt = f"""Rewrite the user's last question clearly and independently, so it doesn't need previous context.
# Conversation history:
# {history_str}

# Last question:
# {user_content}

# Clear rewritten question:"""

#     # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ
#     inputs = tokenizer(
#         prompt,
#         return_tensors="pt",
#         max_length=512,
#         truncation=True,
#         padding="max_length"
#     ).to(device)

#     # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ beam search
#     with torch.no_grad():
#         outputs = model.generate(
#             input_ids=inputs["input_ids"],
#             attention_mask=inputs["attention_mask"],
#             max_length=100,
#             num_beams=4,
#             no_repeat_ngram_size=2,
#             early_stopping=True
#         )

#     rewritten_question = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
#     return rewritten_question