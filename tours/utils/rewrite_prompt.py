import sys
import os

BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(BASE_DIR)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "travelproj.settings")

import django
django.setup()


import openai


# --------------------- ØªÙ†Ø¸ÛŒÙ… OpenAI --------------------- #
openai.api_key = "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"  # <<-- Ú©Ù„ÛŒØ¯Øª Ø§ÛŒÙ†Ø¬Ø§
openai.api_base = "https://openrouter.ai/api/v1"


model = "qwen/qwen3-235b-a22b:free"  # Ù…Ø­Ø´Ø±Ù‡Ù‡Ù‡Ù‡
# model = "openrouter/horizon-beta" # Ø§ÛŒÙ†Ù… Ø¹Ø§Ù„ÛŒÙ‡
# model = "deepseek/deepseek-r1-0528:free"
# model = "z-ai/glm-4.5-air:free"
# model = "google/gemma-3n-e4b-it:free"
# model = "mistralai/mistral-7b-instruct"

# model = "moonshotai/kimi-k2:free"

def rewrite_query_with_context(user_content, history, use_model=True):
    """
    Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.
    
    - history: Ù„ÛŒØ³Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ú¯ÙØªÚ¯Ùˆ (Ù…Ø«Ù„ ["user: ...", "assistant: ..."])
    - use_model: Ø§Ú¯Ø± True Ø¨Ø§Ø´Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ LLM Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´Ù‡
    """
    if not use_model:
        return user_content

    history_str = "\n".join(history[-4:])

    prompt = f"""Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± ÛŒÚ© Ú¯ÙØªÚ¯ÙˆÛŒ Ø³ÙØ± Ø§Ø³Øª. Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±Ø´ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù† Ø¨Ù‡ Ø·ÙˆØ±ÛŒ Ú©Ù‡ Ù…Ø³ØªÙ‚Ù„ Ùˆ ÙˆØ§Ø¶Ø­ Ø¨Ø§Ø´Ø¯ Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù‚Ø¨Ù„ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.

ðŸ•‘ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ:
{history_str}

â“ Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±:
{user_content}

ðŸ” Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ ÙˆØ§Ø¶Ø­:"""

    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„:", e)
        return user_content  # fallback




# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ø² Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ
# model_path = r"C:\Users\Asus\Desktop\test t5\models\bloomz-1b1"
# tokenizer = AutoTokenizer.from_pretrained(model_path)
# model = AutoModelForCausalLM.from_pretrained(model_path)
# model.eval()  # Ø­Ø§Ù„Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ

# def rewrite_query_with_context(user_content, history, use_model=True):
#     """
#     Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.
    
#     - history: Ù„ÛŒØ³Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ú¯ÙØªÚ¯Ùˆ (Ù…Ø«Ù„ ["user: ...", "assistant: ..."])
#     - use_model: Ø§Ú¯Ø± True Ø¨Ø§Ø´Ù‡ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ LLM Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´Ù‡
#     """
#     if not use_model:
#         return user_content

#     history_str = "\n".join(history[-4:])

#     prompt = f"""Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± ÛŒÚ© Ú¯ÙØªÚ¯ÙˆÛŒ Ø³ÙØ± Ø§Ø³Øª. Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±Ø´ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù† Ø¨Ù‡ Ø·ÙˆØ±ÛŒ Ú©Ù‡ Ù…Ø³ØªÙ‚Ù„ Ùˆ ÙˆØ§Ø¶Ø­ Ø¨Ø§Ø´Ø¯ Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù‚Ø¨Ù„ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.

# ðŸ•‘ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ:
# {history_str}

# â“ Ø³ÙˆØ§Ù„ Ø¢Ø®Ø±:
# {user_content}

# ðŸ” Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ ÙˆØ§Ø¶Ø­:"""

#     # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ tensor
#     inputs = tokenizer(prompt, return_tensors="pt")
    
#     # Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ (ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†)
#     with torch.no_grad():
#         outputs = model.generate(
#             **inputs,
#             max_new_tokens=100,
#             do_sample=True,
#             temperature=0.3,
#             top_p=0.9,
#             eos_token_id=tokenizer.eos_token_id,
#             pad_token_id=tokenizer.pad_token_id
#         )
    
#     # Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®
#     generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
#     # Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø® Ø§Ø² prompt (Ù…Ù…Ú©Ù†Ù‡ Ø¨Ø®ÙˆØ§ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¬Ø¯Ø§ Ú©Ù†ÛŒ)
#     answer = generated_text[len(prompt):].strip()

#     return answer


# # ----------mt5--------------------------------------------------------------------------


# from transformers import MT5Tokenizer, TFMT5ForConditionalGeneration
# import tensorflow as tf

# # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„
# model_path = r"C:\Users\Asus\Desktop\test t5\models\mt5-large"

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ (TensorFlow)
# tokenizer = MT5Tokenizer.from_pretrained(model_path)
# model = TFMT5ForConditionalGeneration.from_pretrained(model_path)

# def rewrite_query_with_context(user_content, history, use_model=True):
#     """
#     Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³ÙˆØ§Ù„ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ ØªÙˆØ³Ø· Ù…Ø¯Ù„ MT5 (TensorFlow)

#     Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
#     - user_content (str): Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±
#     - history (list of str): ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ (Ù…Ø«Ù„ ["user: ...", "assistant: ..."])
#     - use_model (bool): Ø§Ú¯Ø± False Ø¨Ø§Ø´Ù‡ Ù‡Ù…ÙˆÙ† Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ Ø±Ùˆ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÙˆÙ†Ù‡

#     Ø®Ø±ÙˆØ¬ÛŒ:
#     - str: Ø³ÙˆØ§Ù„ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒâ€ŒØ´Ø¯Ù‡ ÛŒØ§ Ø³ÙˆØ§Ù„ Ø§ØµÙ„ÛŒ
#     """
#     if not use_model:
#         return user_content

#     # ØªØ±Ú©ÛŒØ¨ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ùˆ Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª
#     history_str = "\n".join(history[-4:])  # Ø¢Ø®Ø±ÛŒÙ† Û´ Ù¾ÛŒØ§Ù…
#     prompt = f"rewrite this user query independently based on the following chat history.\n\nhistory:\n{history_str}\n\nquestion:\n{user_content}\n\nrewritten question:"

#     # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ (max length Ø±Ø¹Ø§ÛŒØª Ø´ÙˆØ¯)
#     inputs = tokenizer(prompt, return_tensors="tf", max_length=512, truncation=True, padding="max_length")

#     # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
#     outputs = model.generate(
#         input_ids=inputs["input_ids"],
#         attention_mask=inputs["attention_mask"],
#         max_length=100,
#         num_beams=4,
#         no_repeat_ngram_size=2,
#         early_stopping=True
#     )

#     # Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø®
#     rewritten = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
#     return rewritten


# # ----------flan--------------------------------------------------------------------------


# from transformers import T5ForConditionalGeneration, T5Tokenizer
# import torch

# # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„
# model_path = r"C:\Users\Asus\Desktop\project-final\medical\models\flan-t5-base"

# # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
# tokenizer = T5Tokenizer.from_pretrained(model_path)
# model = T5ForConditionalGeneration.from_pretrained(model_path)

# # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ GPU Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)
# model.eval()

# def rewrite_query_with_context3(user_content, history, use_model=True):
#     """
#     Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø±Ùˆ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡.
    
#     Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
#         user_content (str): Ø³ÙˆØ§Ù„ Ø¢Ø®Ø± Ú©Ø§Ø±Ø¨Ø±
#         history (list of str): ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ (Ù…Ø«Ù„Ø§ ["user: ...", "assistant: ..."])
#         use_model (bool): Ø§Ú¯Ø± True Ø¨Ø§Ø´Ù‡ØŒ Ø§Ø² Ù…Ø¯Ù„ FLAN-T5 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡ ÙˆÚ¯Ø±Ù†Ù‡ Ø³ÙˆØ§Ù„ Ø±Ùˆ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯ÙˆÙ†Ù‡
    
#     Ø®Ø±ÙˆØ¬ÛŒ:
#         str: Ø³ÙˆØ§Ù„ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ ÛŒØ§ Ø§ØµÙ„ÛŒ
#     """

#     if not use_model:
#         return user_content

#     # ÙÙ‚Ø· Û´ Ù¾ÛŒØ§Ù… Ø¢Ø®Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
#     history_str = "\n".join(history[-4:])

#     prompt = f"""Rewrite the user's last question clearly and independently, so it doesn't need previous context.
# Conversation history:
# {history_str}

# Last question:
# {user_content}

# Clear rewritten question:"""

#     # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ
#     inputs = tokenizer(
#         prompt,
#         return_tensors="pt",
#         max_length=512,
#         truncation=True,
#         padding="max_length"
#     ).to(device)

#     # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ beam search
#     with torch.no_grad():
#         outputs = model.generate(
#             input_ids=inputs["input_ids"],
#             attention_mask=inputs["attention_mask"],
#             max_length=100,
#             num_beams=4,
#             no_repeat_ngram_size=2,
#             early_stopping=True
#         )

#     rewritten_question = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
#     return rewritten_question